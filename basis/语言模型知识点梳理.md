# 语言模型知识点梳理

## 定义

​	**简单地说，语言模型就是用来计算一个句子的概率的模型，也就是判断一句话是否正确（及是否是人话的概率）** 。那么如何计算一个句子的概率呢？假设给定句子（词语序列） S = W1,W2,...,Wk，它的概率可以表示为：

![img](https://pic2.zhimg.com/80/v2-e8e7c61133d1b23e4d869352aae0c455_hd.png)

可是这样的方法存在两个致命的缺陷：

1. 参数空间过大：添加概率 P(wn | w1,w2,...,wn-1) 的可能性太多，无法估算，不可能有用；
2. 数据稀疏严重：对于非常多词对的组合，在语料库中都没有出现，依据最大似然估计得到的概率将会是 0



## 马尔可夫假设

​	为了解决参数空间过大的问题。引入了马尔可夫假设：任意一个词出现的概率只与它前面出现的有限的一个或者几个词有关。这就出现了 N-gram 模型，即如下列举：

1. 如果一个词的出现与它周围的词是独立的，那么我们就称之为 unigram 也就是一元语言模型：

   ![img](https://pic3.zhimg.com/80/v2-dfb6d0be8fa42f803d45e27cb02acf5e_hd.png)

2. 如果一个词的出现仅依赖于它前面出现的一个词，那么我们就称之为 bigram 也就是二元语言模型：

   ![img](https://pic1.zhimg.com/80/v2-f0e63faeed0dbde5219a3e09778e5b0c_hd.png)

3. 如果一个词的出现仅依赖于它前面出现的两个词，那么我们就称之为trigram 也就是三元语言模型：

   ![img](https://pic3.zhimg.com/80/v2-4c4b2b156e248bc0dea8812b2b5f0002_hd.png)

   **一般来说，N 元模型就是假设当前词的出现概率只与它前面的 N - 1 个词有关。而这些概率参数都是可以通过大规模语料库来计算，比如三元概率有：**

   ![img](https://pic2.zhimg.com/80/v2-4cd447daf3199099507d4d8f091e630d_hd.png)

   在实践中用的最多的就是 **bigram** 和 **trigram** 了，高于四元的用的非常少，由于训练它要更庞大的语料，并且数据稀疏严重，时间复杂度高，精度却要提高的不多。

## 一元语言模型

​	一元模型中就是假设语言序列中每个单词都互不相关，即概率计算公式为：

![p(s)=p(w1)*p(w2)*p(w3)*....*p(wn)](https://www.zhihu.com/equation?tex=p%28s%29%3Dp%28w1%29%2Ap%28w2%29%2Ap%28w3%29%2A....%2Ap%28wn%29)

​	而如何计算![p(w1)*p(w2)*p(w3)*....*p(wn)](https://www.zhihu.com/equation?tex=p%28w1%29%2Ap%28w2%29%2Ap%28w3%29%2A....%2Ap%28wn%29) 而这些值每个单词在语料库出现的概率。但是其实假设每个单词互不相关是不正确，举例如下：`我爱` 后面跟着 `你` 这个字的概率是要比出现 `死` 的概率要大。



## 二元语言模型

​	那么上述一元模型无法解决单词之间的关系的问题，就可以通过二元模型来解决。那么二元语言模型的定义为：

![p(s)=p(w1|</s>)*p(w2|w1)*p(w3|w2)*....*p(</s>|wn)](https://www.zhihu.com/equation?tex=p%28s%29%3Dp%28w1%7C%3C%2Fs%3E%29%2Ap%28w2%7Cw1%29%2Ap%28w3%7Cw2%29%2A....%2Ap%28%3C%2Fs%3E%7Cwn%29)

举例如下：

P(\<s>  I want english food  \</s>) = P(I|\<s>) × P(want|I) × P(english|want) × P(food|english) × P(\</s>|food)

​	那么我们就能通过一个二元语言模型来计算所需要求的句子的概率大小。而且**二元语言模型也能比一元语言模型能够更好的获取到两个词语直接的关系信息**。

