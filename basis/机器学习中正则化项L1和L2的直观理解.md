#  机器学习中正则化项L1和L2的直观理解

## 1. 正则化概念

​	机器学习中几乎都可以看到损失函数后面会添加一个额外项，常用的额外项一般有两种，一般英文称作 ℓ1-norm 和 ℓ2 -norm，中文称作 **L1正则化** 和 **L2正则化**，或者 **L1范数** 和 **L2范数**。

​	L1正则化和L2正则化可以看做是损失函数的惩罚项。所谓『惩罚』是指对损失函数中的某些参数做一些限制。对于线性回归模型，使用L1正则化的模型建叫做Lasso回归，使用L2正则化的模型叫做Ridge回归（岭回归）。

​	下图是Python中Lasso回归的损失函数，式中加号后面一项 $$\alpha||w||_1$$  即为L1正则化项。
$$
min_w \frac{1}{2n_{sample}}||Xw-y||_2^2+\alpha||w||_1
$$
 	下图是 Python 中 Ridge 回归的损失函数，式中加号后面一项 $$\alpha||w||_2^2$$  即为 L2 正则化项
$$
min_w ||Xw-y||_2^2 + \alpha||w||_2^2
$$
​	一般回归分析中回归w ww表示特征的系数，从上式可以看到正则化项是对系数做了处理（限制）。L1正则化和L2正则化的说明如下：

- L1正则化是指权值向量w ww中各个元素的绝对值之和，通常表示为$$||w||_1$$

- L2正则化是指权值向量w ww中各个元素的平方和然后再求平方根（可以看到Ridge回归的L2正则化项有平方符号），通常表示为$$||w||_2$$


​	一般都会在正则化项之前添加一个系数，Python中用α \alphaα表示，一些文章也用λ \lambdaλ表示。这个系数需要用户指定。那添加L1和L2正则化有什么用？下面是L1正则化和L2正则化的作用，这些表述可以在很多文章中找到。

- L1正则化可以产生稀疏权值矩阵，即产生一个稀疏模型，可以用于特征选择
- L2正则化可以防止模型过拟合（overfitting）；一定程度上，L1也可以防止过拟合

### 1.1. 模型稀疏和特征选择

​	上面提到L1正则化有助于生成一个稀疏权值矩阵，进而可以用于特征选择。为什么要生成一个稀疏矩阵？

​	稀疏矩阵指的是很多元素为0，只有少数元素是非零值的矩阵，即得到的线性回归模型的大部分系数都是0. 通常机器学习中特征数量很多，例如文本处理时，如果将一个词组（term）作为一个特征，那么特征数量会达到上万个（bigram）。在预测或分类时，那么多特征显然难以选择，但是如果代入这些特征得到的模型是一个稀疏模型，表示只有少数特征对这个模型有贡献，绝大部分特征是没有贡献的，或者贡献微小（因为它们前面的系数是0或者是很小的值，即使去掉对模型也没有什么影响），此时我们就可以只关注系数是非零值的特征。这就是稀疏模型与特征选择的关系。

## 2. L1和L2正则化的直观理解

​	这部分内容将解释**为什么L1正则化可以产生稀疏模型（L1是怎么让系数等于零的）**，以及**为什么L2正则化可以防止过拟合**。

### 2.1 正则化和特征选择

假设有如下带L1正则化的损失函数：
$$
J = J_0 + \alpha \sum_w{|w|} \tag{1}
$$
​	其中$$J_0$$是原始的损失函数，加号后面的一项是 L1 正则化项，$$\alpha$$ 是正则化系数。注意到 L1 正则化是权值的绝对值之和，$$J$$是带有绝对值符号的函数，因此$$J$$是不完全可微的。机器学习的任务就是要通过一些方法（比如梯度下降）求出损失函数的最小值。当我们在原始损失函数$$J_0$$ 后添加L1正则化项时，相当于对 $$ J_0 $$  做了一个约束。令 $$L = \alpha \sum_w{|w|}$$，则$$J = J_0 + L$$，此时我们的任务变成在 L 约束下求出取 $$ J_0 $$ 最小值的解。考虑二维的情况，即只有两个权值$$w^1$$和$$w^2$$，此时 $$L = |w^1| + |w^2|$$ 对于梯度下降法，求解 $$J_0$$ 的过程可以画出等值线，同时L1正则化的函数 $$ L1 $$ 也可以在$$w^1w^2$$的二维平面上画出来。如下图：

![20160904184428459](../images/Regularization/20160904184428459.jpg)