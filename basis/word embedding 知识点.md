# word embedding 知识点

## 定义

​	embedding是数学领域的专有名词，是指某个对象 X 被嵌入到另外一个对象 Y 中，映射 f:X -> Y，例如有理数嵌入实数。而 word embedding 是 NLP 中一组语言模型与特征学习技术的总称，把词汇表中的单词或者短语映射成由实数构成的向量上（映射）。

## 传统场景下训练 word embedding 的方法

1. 基于词袋（BOW）的 One-Hot 表示，这种方法把词汇表中的词排成一列，对于某个单词A，如果它出现在上述词汇序列中的位置为 K，那么它的向量表示就是“第K位为1，其他位置都为0”的一个向量。

   缺点：1、没有考虑单词之间相对位置的关系；2、词向量可能非常非常长。

2. 共现矩阵：假设某个词的意思跟它临近的单词是紧密相关的。这是我们可以设定一个窗口（大小一般是5~10），然后利用这种共现关系来生成词向量。

   缺点：1、虽解决了单词相对位置的问题，但仍然面对维度灾难，也就是一个 word 的向量表示长度太长了。

# word2vec 知识点

​	前面已经提到了两个传统机器学习方法训练word embedding的方法，下面介绍一种基于神经网络来训练word embedding的方法：word2vec 是由 Tomas Mikolov在谷歌工作时发明的一类方法，也是由谷歌开源的一个工具包的名称。word2vec 包含了两个训练算法：CBOW 和 skip-gram。

​	word2vec 是从大量文本语料中以无监督的方式学习语义知识的一种模型，被大量地用在自然语言处理中。word2vec 其实就是通过学习文本来用词向量的方式表征词的语义信息，即通过一个嵌入空间使得语义上相似的单词在该空间内举例很近。

​	word2vec 之所以现在这么流行，不同于之前 word embedding 方法，它能够自动实现：1）单词语义相似性的度量；2）词汇的语义的类比。

​	从直观上理解，skip-gram 是给定 input word 来预测上下文。而 CBOW 是给定上下文，来预测 input word。具体展示如下：

![ä¸æè¯¦è§£ Word2vec ä¹ Skip-Gram æ¨¡åï¼ç"æç¯ï¼](https://static.leiphone.com/uploads/new/article/740_740/201706/594b306c8b3b1.png?imageMogr2/format/jpg/quality/90)

​	Word2Vec模型实际上分为了两个部分，**第一部分为建立模型，第二部分是通过模型获取嵌入词向量**。Word2Vec的整个建模过程实际上与自编码器（auto-encoder）的思想很相似，即先基于训练数据构建一个神经网络，当这个模型训练好以后，我们并不会用这个训练好的模型处理新的任务，我们真正需要的是这个模型通过训练数据所学得的参数，例如隐层的权重矩阵——后面我们将会看到这些权重在Word2Vec中实际上就是我们试图去学习的“word vectors”。基于训练数据建模的过程，我们给它一个名字叫“Fake Task”，意味着建模并不是我们最终的目的。

> 上面提到的这种方法实际上会在无监督特征学习（unsupervised feature learning）中见到，最常见的就是自编码器（auto-encoder）：通过在隐层将输入进行编码压缩，继而在输出层将数据解码恢复初始状态，训练完成后，我们会将输出层“砍掉”，仅保留隐层。

## skip-gram

### 如何训练神经网络

假如我们有一个句子**“The dog barked at the mailman”**。

- 首先我们选句子中间的一个词作为我们的输入词，例如我们选取“dog”作为input word；
- 有了 input word 以后，我们再定义一个叫做 skip_window 的参数，它代表着我们从当前 input word 的一侧（左边或右边）选取词的数量。如果我们设置 skip_window=2，那么我们最终获得窗口中的词（包括 input word 在内）就是**['The', 'dog'，'barked', 'at']**。skip_window=2 代表着选取左 input word 左侧2个词和右侧2个词进入我们的窗口，所以整个窗口大小 span=2x2=4。另一个参数叫 num_skips，它代表着我们从整个窗口中选取多少个不同的词作为我们的 output word，当 skip_window=2，num_skips=2 时，我们将会得到两组 **(input word, output word)** 形式的训练数据，即 **('dog', 'barked')**，**('dog', 'the')**。
- 神经网络基于这些训练数据将会输出一个概率分布，这个概率代表着我们的词典中的每个词是 output word 的可能性。这句话有点绕，我们来看个栗子。第二步中我们在设置 skip_window 和 num_skips=2 的情况下获得了两组训练数据。假如我们先拿一组数据 **('dog', 'barked')** 来训练神经网络，那么模型通过学习这个训练样本，会告诉我们词汇表中每个单词是 “barked” 的概率大小。

**模型的输出概率代表着到我们词典中每个词有多大可能性跟input word同时出现**。下面的图中给出了一些我们的训练样本的例子。我们选定句子**“The quick brown fox jumps over lazy dog”**，设定我们的窗口大小为2（window_size=2），也就是说我们仅选输入词前后各两个词和输入词进行组合。下图中，蓝色代表input word，方框内代表位于窗口内的单词。

![ä¸æè¯¦è§£ Word2vec ä¹ Skip-Gram æ¨¡åï¼ç"æç¯ï¼](https://static.leiphone.com/uploads/new/article/740_740/201706/594b319eb5f1f.png?imageMogr2/format/jpg/quality/90)

### 模型细节

![ä¸æè¯¦è§£ Word2vec ä¹ Skip-Gram æ¨¡åï¼ç"æç¯ï¼](https://static.leiphone.com/uploads/new/article/740_740/201706/594b31d0920ef.png?imageMogr2/format/jpg/quality/90)

​	skip-gram 的训练模型共分三层，分别为输入层、隐层和输出层，下面分成三个部分进行讲解。

#### 输入层

1. **如何表示单词**：基于训练文档（语料库）来构建我们自己的词汇表（vocabulary）再对单词进行 one-hot 编码；

   > 假设从我们的训练文档中抽取出10000个唯一不重复的单词组成词汇表。我们对这10000个单词进行one-hot编码，得到的每个单词都是一个10000维的向量，向量每个维度的值只有0或者1，假如单词ants在词汇表中的出现位置为第3个，那么ants的向量就是一个第三维度取值为1，其他维都为0的10000维的向量（ants=[0, 0, 1, 0, ..., 0]）。

2. 模型的输入如果为一个10000维的向量，那么输出也是一个10000维度（词汇表的大小）的向量，它包含了10000个概率，每一个概率代表着当前词是输入样本中output word的概率大小。

3. 我们基于成对的单词来对神经网络进行训练，训练样本是 ( input word, output word ) 这样的单词对，input word和output word都是one-hot编码的向量。最终模型的输出是一个概率分布。

#### 隐层

1. 隐层没有使用任何激活函数；

2. 如果我们现在想用300个特征来表示一个单词（即每个词可以被表示为300维的向量）。那么隐层的权重矩阵应该为10000行，300列（隐层有300个结点）；

3. 下面图片是不同角度下输入层-隐层的权重矩阵的解释：左图中每一列代表一个10000维的词向量和隐层单个神经元连接的权重向量；右图中每一行实际上代表了每个单词的词向量。

   ![ä¸æè¯¦è§£ Word2vec ä¹ Skip-Gram æ¨¡åï¼ç"æç¯ï¼](https://static.leiphone.com/uploads/new/article/740_740/201706/594b320f8ed60.png?imageMogr2/format/jpg/quality/90)

4. 10000维下的矩阵运算是十分低效的。为了有效地进行计算，这种稀疏状态下不会进行矩阵乘法计算，可以看到矩阵的计算的结果实际上是矩阵对应的向量中值为1 的索引，这样模型中的隐层权重矩阵便成了一个“查找表”，进行矩阵运算，直接去查输入向量中取值为1的维度下对应的那些权重值。

5. 隐层的输出就是每个输入单词的“嵌入词向量”。

#### 输出层

1. 经过神经网络隐层的计算，单词向量从一个1 x 10000 的向量变成 1x 300 的向量，再被输入到输出层中计算；

2. 输出层就是一个softmax回归分类器，它的每个结点将会输出一个 0-1 之间的值（概率），这些所有输出层神经元结点的概率之和为1：

   ![ä¸æè¯¦è§£ Word2vec ä¹ Skip-Gram æ¨¡åï¼ç"æç¯ï¼](https://static.leiphone.com/uploads/new/article/740_740/201706/594b3267c64f4.png?imageMogr2/format/jpg/quality/90)

